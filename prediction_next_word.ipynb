{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7e56fec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     C:\\Users\\anial\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "## Data Collection\n",
    "import nltk\n",
    "nltk.download('gutenberg')\n",
    "from nltk.corpus import gutenberg\n",
    "import  pandas as pd\n",
    "\n",
    "## load the dataset\n",
    "data=gutenberg.raw('shakespeare-hamlet.txt')\n",
    "## save to a file\n",
    "with open('hamlet.txt','w') as file:\n",
    "    file.write(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42cd56d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preprocessing\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "367e989d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "with open('hamlet.txt','r') as file:\n",
    "    text=file.read().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b13c7069",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4818"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize the text\n",
    "tokenizer=Tokenizer()\n",
    "tokenizer.fit_on_texts([text])\n",
    "total_words=len(tokenizer.word_index)+1\n",
    "total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d50ec54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create input sequences\n",
    "inputsequences = []\n",
    "for line in text.split('\\n'):\n",
    "    token_list=tokenizer.texts_to_sequences([line])[0]\n",
    "    for i in range(1,len(token_list)):\n",
    "        n_gram_sequence=token_list[:i+1]\n",
    "        inputsequences.append(n_gram_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f2dd8e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pad Sequences\n",
    "max_sequence_len=max([len(x) for x in inputsequences])\n",
    "max_sequence_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46c17b5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0, ...,    0,    1,  687],\n",
       "       [   0,    0,    0, ...,    1,  687,    4],\n",
       "       [   0,    0,    0, ...,  687,    4,   45],\n",
       "       ...,\n",
       "       [   0,    0,    0, ...,    4,   45, 1047],\n",
       "       [   0,    0,    0, ...,   45, 1047,    4],\n",
       "       [   0,    0,    0, ..., 1047,    4,  193]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputsequences=np.array(pad_sequences(inputsequences,maxlen=max_sequence_len,padding='pre'))\n",
    "inputsequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b9bd3c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Predictors and label\n",
    "import tensorflow as tf\n",
    "x,y=inputsequences[:,:-1],inputsequences[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b248d9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = tf.keras.utils.to_categorical(y,num_classes=total_words)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e810c870",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c33d4566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     (None, 13, 100)           481800    \n",
      "                                                                 \n",
      " lstm_4 (LSTM)               (None, 13, 150)           150600    \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 13, 150)           0         \n",
      "                                                                 \n",
      " lstm_5 (LSTM)               (None, 100)               100400    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 4818)              486618    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1219418 (4.65 MB)\n",
      "Trainable params: 1219418 (4.65 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Train LSTM RNN\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding,LSTM,Dense,Dropout\n",
    "\n",
    "# Define model\n",
    "model=Sequential()\n",
    "model.add(Embedding(total_words,100,input_length=max_sequence_len-1))\n",
    "model.add(LSTM(150,return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(total_words,activation='softmax'))\n",
    "\n",
    "# Model Compile\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "09d7a233",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:From d:\\Projects\\Predict_Next_Word\\venv\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From d:\\Projects\\Predict_Next_Word\\venv\\lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "644/644 [==============================] - 30s 33ms/step - loss: 6.9060 - accuracy: 0.0320 - val_loss: 6.7969 - val_accuracy: 0.0326\n",
      "Epoch 2/50\n",
      "644/644 [==============================] - 20s 31ms/step - loss: 6.4613 - accuracy: 0.0379 - val_loss: 6.8931 - val_accuracy: 0.0414\n",
      "Epoch 3/50\n",
      "644/644 [==============================] - 20s 31ms/step - loss: 6.3249 - accuracy: 0.0466 - val_loss: 6.9389 - val_accuracy: 0.0474\n",
      "Epoch 4/50\n",
      "644/644 [==============================] - 19s 30ms/step - loss: 6.1724 - accuracy: 0.0516 - val_loss: 6.9685 - val_accuracy: 0.0427\n",
      "Epoch 5/50\n",
      "644/644 [==============================] - 20s 31ms/step - loss: 6.0284 - accuracy: 0.0553 - val_loss: 6.9919 - val_accuracy: 0.0538\n",
      "Epoch 6/50\n",
      "644/644 [==============================] - 19s 29ms/step - loss: 5.8786 - accuracy: 0.0629 - val_loss: 7.0234 - val_accuracy: 0.0589\n",
      "Epoch 7/50\n",
      "644/644 [==============================] - 18s 28ms/step - loss: 5.7298 - accuracy: 0.0730 - val_loss: 7.0791 - val_accuracy: 0.0637\n",
      "Epoch 8/50\n",
      "644/644 [==============================] - 19s 29ms/step - loss: 5.5823 - accuracy: 0.0809 - val_loss: 7.1640 - val_accuracy: 0.0682\n",
      "Epoch 9/50\n",
      "644/644 [==============================] - 20s 31ms/step - loss: 5.4444 - accuracy: 0.0857 - val_loss: 7.2669 - val_accuracy: 0.0661\n",
      "Epoch 10/50\n",
      "644/644 [==============================] - 18s 28ms/step - loss: 5.3135 - accuracy: 0.0934 - val_loss: 7.3402 - val_accuracy: 0.0647\n",
      "Epoch 11/50\n",
      "644/644 [==============================] - 17s 27ms/step - loss: 5.1898 - accuracy: 0.1004 - val_loss: 7.4738 - val_accuracy: 0.0668\n",
      "Epoch 12/50\n",
      "644/644 [==============================] - 18s 27ms/step - loss: 5.0715 - accuracy: 0.1039 - val_loss: 7.5656 - val_accuracy: 0.0663\n",
      "Epoch 13/50\n",
      "644/644 [==============================] - 21s 32ms/step - loss: 4.9520 - accuracy: 0.1114 - val_loss: 7.6577 - val_accuracy: 0.0672\n",
      "Epoch 14/50\n",
      "644/644 [==============================] - 21s 32ms/step - loss: 4.8405 - accuracy: 0.1133 - val_loss: 7.7961 - val_accuracy: 0.0674\n",
      "Epoch 15/50\n",
      "644/644 [==============================] - 21s 32ms/step - loss: 4.7319 - accuracy: 0.1204 - val_loss: 7.8947 - val_accuracy: 0.0637\n",
      "Epoch 16/50\n",
      "644/644 [==============================] - 21s 32ms/step - loss: 4.6229 - accuracy: 0.1243 - val_loss: 8.0556 - val_accuracy: 0.0643\n",
      "Epoch 17/50\n",
      "644/644 [==============================] - 21s 32ms/step - loss: 4.5149 - accuracy: 0.1322 - val_loss: 8.1686 - val_accuracy: 0.0651\n",
      "Epoch 18/50\n",
      "644/644 [==============================] - 21s 32ms/step - loss: 4.4082 - accuracy: 0.1392 - val_loss: 8.2974 - val_accuracy: 0.0618\n",
      "Epoch 19/50\n",
      "644/644 [==============================] - 21s 32ms/step - loss: 4.3055 - accuracy: 0.1445 - val_loss: 8.4636 - val_accuracy: 0.0643\n",
      "Epoch 20/50\n",
      "644/644 [==============================] - 20s 31ms/step - loss: 4.2075 - accuracy: 0.1581 - val_loss: 8.5740 - val_accuracy: 0.0641\n",
      "Epoch 21/50\n",
      "644/644 [==============================] - 20s 31ms/step - loss: 4.1160 - accuracy: 0.1686 - val_loss: 8.7344 - val_accuracy: 0.0635\n",
      "Epoch 22/50\n",
      "644/644 [==============================] - 20s 31ms/step - loss: 4.0251 - accuracy: 0.1812 - val_loss: 8.8508 - val_accuracy: 0.0593\n",
      "Epoch 23/50\n",
      "644/644 [==============================] - 21s 32ms/step - loss: 3.9418 - accuracy: 0.1959 - val_loss: 9.0299 - val_accuracy: 0.0622\n",
      "Epoch 24/50\n",
      "644/644 [==============================] - 20s 31ms/step - loss: 3.8549 - accuracy: 0.2083 - val_loss: 9.1612 - val_accuracy: 0.0622\n",
      "Epoch 25/50\n",
      "644/644 [==============================] - 20s 31ms/step - loss: 3.7799 - accuracy: 0.2196 - val_loss: 9.2856 - val_accuracy: 0.0573\n",
      "Epoch 26/50\n",
      "644/644 [==============================] - 19s 30ms/step - loss: 3.6985 - accuracy: 0.2326 - val_loss: 9.3703 - val_accuracy: 0.0626\n",
      "Epoch 27/50\n",
      "644/644 [==============================] - 18s 28ms/step - loss: 3.6310 - accuracy: 0.2460 - val_loss: 9.5141 - val_accuracy: 0.0612\n",
      "Epoch 28/50\n",
      "644/644 [==============================] - 18s 29ms/step - loss: 3.5620 - accuracy: 0.2579 - val_loss: 9.6258 - val_accuracy: 0.0595\n",
      "Epoch 29/50\n",
      "644/644 [==============================] - 19s 29ms/step - loss: 3.4958 - accuracy: 0.2682 - val_loss: 9.7549 - val_accuracy: 0.0591\n",
      "Epoch 30/50\n",
      "644/644 [==============================] - 19s 29ms/step - loss: 3.4341 - accuracy: 0.2788 - val_loss: 9.8338 - val_accuracy: 0.0602\n",
      "Epoch 31/50\n",
      "644/644 [==============================] - 19s 30ms/step - loss: 3.3759 - accuracy: 0.2876 - val_loss: 9.9599 - val_accuracy: 0.0604\n",
      "Epoch 32/50\n",
      "644/644 [==============================] - 20s 31ms/step - loss: 3.3185 - accuracy: 0.2976 - val_loss: 10.0562 - val_accuracy: 0.0589\n",
      "Epoch 33/50\n",
      "644/644 [==============================] - 20s 32ms/step - loss: 3.2588 - accuracy: 0.3097 - val_loss: 10.1419 - val_accuracy: 0.0585\n",
      "Epoch 34/50\n",
      "644/644 [==============================] - 21s 32ms/step - loss: 3.2090 - accuracy: 0.3178 - val_loss: 10.2312 - val_accuracy: 0.0560\n",
      "Epoch 35/50\n",
      "644/644 [==============================] - 18s 27ms/step - loss: 3.1540 - accuracy: 0.3286 - val_loss: 10.3564 - val_accuracy: 0.0571\n",
      "Epoch 36/50\n",
      "644/644 [==============================] - 18s 28ms/step - loss: 3.1036 - accuracy: 0.3369 - val_loss: 10.4253 - val_accuracy: 0.0560\n",
      "Epoch 37/50\n",
      "644/644 [==============================] - 18s 28ms/step - loss: 3.0566 - accuracy: 0.3440 - val_loss: 10.4763 - val_accuracy: 0.0538\n",
      "Epoch 38/50\n",
      "644/644 [==============================] - 18s 28ms/step - loss: 3.0124 - accuracy: 0.3526 - val_loss: 10.6003 - val_accuracy: 0.0575\n",
      "Epoch 39/50\n",
      "644/644 [==============================] - 18s 27ms/step - loss: 2.9632 - accuracy: 0.3595 - val_loss: 10.6873 - val_accuracy: 0.0540\n",
      "Epoch 40/50\n",
      "644/644 [==============================] - 18s 28ms/step - loss: 2.9171 - accuracy: 0.3696 - val_loss: 10.7458 - val_accuracy: 0.0536\n",
      "Epoch 41/50\n",
      "644/644 [==============================] - 18s 27ms/step - loss: 2.8803 - accuracy: 0.3740 - val_loss: 10.7952 - val_accuracy: 0.0523\n",
      "Epoch 42/50\n",
      "644/644 [==============================] - 18s 27ms/step - loss: 2.8369 - accuracy: 0.3823 - val_loss: 10.8823 - val_accuracy: 0.0536\n",
      "Epoch 43/50\n",
      "644/644 [==============================] - 18s 27ms/step - loss: 2.7941 - accuracy: 0.3928 - val_loss: 10.9951 - val_accuracy: 0.0521\n",
      "Epoch 44/50\n",
      "644/644 [==============================] - 18s 28ms/step - loss: 2.7558 - accuracy: 0.3981 - val_loss: 11.0700 - val_accuracy: 0.0519\n",
      "Epoch 45/50\n",
      "644/644 [==============================] - 17s 27ms/step - loss: 2.7127 - accuracy: 0.4052 - val_loss: 11.1514 - val_accuracy: 0.0509\n",
      "Epoch 46/50\n",
      "644/644 [==============================] - 18s 28ms/step - loss: 2.6812 - accuracy: 0.4119 - val_loss: 11.1907 - val_accuracy: 0.0534\n",
      "Epoch 47/50\n",
      "644/644 [==============================] - 18s 28ms/step - loss: 2.6406 - accuracy: 0.4198 - val_loss: 11.2548 - val_accuracy: 0.0527\n",
      "Epoch 48/50\n",
      "644/644 [==============================] - 18s 27ms/step - loss: 2.6048 - accuracy: 0.4275 - val_loss: 11.3434 - val_accuracy: 0.0499\n",
      "Epoch 49/50\n",
      "644/644 [==============================] - 18s 28ms/step - loss: 2.5675 - accuracy: 0.4347 - val_loss: 11.4024 - val_accuracy: 0.0521\n",
      "Epoch 50/50\n",
      "644/644 [==============================] - 18s 28ms/step - loss: 2.5371 - accuracy: 0.4405 - val_loss: 11.4950 - val_accuracy: 0.0515\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "history = model.fit(x_train,y_train,epochs=50,validation_data=(x_test,y_test),verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "80a13e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to predict the next word\n",
    "def predict_next_word(model, tokenizer, text, max_sequence_len):\n",
    "    token_list = tokenizer.texts_to_sequences([text])[0]\n",
    "    if len(token_list) >= max_sequence_len:\n",
    "        token_list = token_list[-(max_sequence_len-1):]  # Ensure the sequence length matches max_sequence_len-1\n",
    "    token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "    predicted = model.predict(token_list, verbose=0)\n",
    "    predicted_word_index = np.argmax(predicted, axis=1)\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == predicted_word_index:\n",
    "            return word\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b2f54d0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text:To be or not to be\n",
      "Next Word PRediction:in't\n"
     ]
    }
   ],
   "source": [
    "input_text=\"To be or not to be\"\n",
    "print(f\"Input text:{input_text}\")\n",
    "max_sequence_len=model.input_shape[1]+1\n",
    "next_word=predict_next_word(model,tokenizer,input_text,max_sequence_len)\n",
    "print(f\"Next Word PRediction:{next_word}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5740c1c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Projects\\Predict_Next_Word\\venv\\lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "## Save the model\n",
    "model.save(\"next_word_lstm.h5\")\n",
    "## Save the tokenizer\n",
    "import pickle\n",
    "with open('tokenizer.pickle','wb') as handle:\n",
    "    pickle.dump(tokenizer,handle,protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d742e669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text:Fran. You come most carefully vpon your\n",
      "Next Word PRediction:maiestie\n"
     ]
    }
   ],
   "source": [
    "input_text=\"Fran. You come most carefully vpon your\"\n",
    "print(f\"Input text:{input_text}\")\n",
    "max_sequence_len=model.input_shape[1]+1\n",
    "next_word=predict_next_word(model,tokenizer,input_text,max_sequence_len)\n",
    "print(f\"Next Word PRediction:{next_word}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da9ca7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
